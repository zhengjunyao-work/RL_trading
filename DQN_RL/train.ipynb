{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import importlib\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from agents.DQN import Agent\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from utils import Portfolio\n",
    "\n",
    "class Agent(Portfolio):\n",
    "    def __init__(self,state_dim,balance,is_eval=False,model_name=\"\"):\n",
    "        super().__init__(balance=balance)\n",
    "        self.model_type=\"DQN\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = 3\n",
    "        self.memory=deque(maxlen=100)\n",
    "        self.buffer_size=60\n",
    "\n",
    "        self.gamma=0.95\n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_min= 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.is_eval = is_eval\n",
    "        self.model = load_model(\"save_models/{}.h5\".format(model_name)) if is_eval else self.model()\n",
    "        self.print_f()\n",
    "        \n",
    "        \n",
    "    def print_f(self):\n",
    "        print(\"hello world.\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def model(self):\n",
    "        print(\"test\")\n",
    "        model=Sequential()\n",
    "        model.add(Dense(units=64,input_dim = self.state_dim,activation=\"relu\"))\n",
    "        model.add(Dense(units =32,activation=\"relu\"))\n",
    "        model.add(Dense(units = 8,activation='relu'))\n",
    "        model.add(Dense(self.action_dim,activation='softmax'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.01))\n",
    "        return model\n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_portfolio()\n",
    "        self.epsilon=1.0\n",
    "\n",
    "    def remember(self,state,actions,reward,next_state,done):\n",
    "        self.memory.append((state,actions,reward,next_state,done))\n",
    "\n",
    "    def act(self,state):\n",
    "        if not self.is_eval and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        options = self.model.predict(state)\n",
    "        return np.argmax(options[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "\n",
    "        mini_batch = [self.memory[i] for i in range(len(self.memory)-self.buffer_size+1,len(self.memory))]\n",
    "\n",
    "        for state, actions, reward, next_state, done in mini_batch:\n",
    "            if not done:\n",
    "                Q_target_value = reward+self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                Q_target_value = reward\n",
    "\n",
    "            next_actions = self.model.predict(state)\n",
    "            next_actions[0][np.argmax(actions)]=Q_target_value\n",
    "            history = self.model.fit(state,next_actions,epochs=1,verbose =1)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return history.history['loss'][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "hello world.\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"command line options\")\n",
    "# parser.add_argument(\"--model_name\",action=\"store\",dest=\"model_name\",default=\"DQN\",help='model_name')\n",
    "# parser.add_argument('--stock_name',action=\"store\",dest='stock_name',default='^GSPC_2010-2015',help=\"stock_name\")\n",
    "# parser.add_argument('--window_size',action=\"store\",dest='window_size',default='10',type =int ,help=\"span (days) of observation\")\n",
    "# parser.add_argument('--num_episode',action=\"store\",dest='num_episode',default='10',type =int ,help=\"episode number\")\n",
    "# parser.add_argument('--initial_balance',action=\"store\",dest='initial_balance',default='50000',help=\"initial balance\")\n",
    "\n",
    "# inputs = parser.parse_args()\n",
    "\n",
    "# model_name = inputs.model_name\n",
    "# stock_name = inputs.stock_name\n",
    "# window_size= inputs.window_size\n",
    "# num_episode =inputs.num_episode\n",
    "# initial_balance=inputs.initial_balance\n",
    "\n",
    "\n",
    "model_name = \"DQN\"\n",
    "stock_name = '^GSPC_2010-2015'\n",
    "window_size= 10\n",
    "num_episode =10\n",
    "initial_balance=50000\n",
    "\n",
    "\n",
    "stock_prices = stock_close_prices(stock_name)\n",
    "trading_period = len(stock_prices) -1\n",
    "returns_across_episodes=[]\n",
    "num_experience_replay=0\n",
    "action_dict={0:\"Hold\",1:\"Buy\",2:\"Sell\"}\n",
    "\n",
    "\n",
    "agent = Agent(state_dim=window_size +3, balance= initial_balance)\n",
    "\n",
    "\n",
    "def hold(actions):\n",
    "    # encourage selling for profit and liquidity\n",
    "    next_probable_action = np.argsort(actions)[1]\n",
    "    if next_probable_action==2 and len(agent.inventory) >0:\n",
    "        max_profit = stock_prices[t]  - agent.inventory[0]\n",
    "        if max_profit >0:\n",
    "            sell(t)\n",
    "            actions[next_probable_action] = 1 #reset this action's value to the highest\n",
    "            return \"Hold\",actions\n",
    "\n",
    "\n",
    "def buy(t):\n",
    "    if agent.balance > stock_prices[t]:\n",
    "        agent.balance-=stock_prices[t]\n",
    "        agent.inventory.add(stock_prices[t])\n",
    "        return 'Buy: ${:.2f}'.format(stock_prices[t])\n",
    "\n",
    "def sell(t):\n",
    "    if len(agent.inventory)>0:\n",
    "        agent.balance+=stock_prices[t]\n",
    "\n",
    "        #应该pop 最便宜的inventory\n",
    "        bought_price = agent.inventory.pop(0)\n",
    "        profit = stock_prices[t] - bought_price\n",
    "        global reward\n",
    "\n",
    "        reward = profit\n",
    "\n",
    "        return \"Sell: ${:.2f} | Profit: ${:.2f}\".format(stock_prices[t],profit)\n",
    "\n",
    "\n",
    "logging.basicConfig(filename=f'log/{model_name}_training_{stock_name}.log',filemode=\"w\",\n",
    "format='[%(asctime)s.%(msecs)03d %(filename)s:%(lineno)3s] %(message)s',\n",
    "datefmt='%m/%d/%Y %H:%M:%S',level = logging.INFO)\n",
    "\n",
    "logging.info(f'Trading Object:              {stock_name}')\n",
    "logging.info(f'Trading Period:              {trading_period} days')\n",
    "logging.info(f'Window Size:              {window_size} days')\n",
    "logging.info(f'Training Episode:              {num_episode}')\n",
    "logging.info(f'Model Name:              {model_name}')\n",
    "logging.info(f'Initial Portfolio Value: ${initial_balance}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [0.04918071 0.51252866 0.43829057]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Precision not allowed in integer format specifier",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alex/Documents/GitHub/RL_trading/DQN_RL/train.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/Documents/GitHub/RL_trading/DQN_RL/train.ipynb#ch0000002?line=20'>21</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/Documents/GitHub/RL_trading/DQN_RL/train.ipynb#ch0000002?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(t,actions)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alex/Documents/GitHub/RL_trading/DQN_RL/train.ipynb#ch0000002?line=22'>23</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39;49m\u001b[39mStep: 『』\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39mHold signal: \u001b[39;49m\u001b[39m{:.4}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m Buy signal: \u001b[39;49m\u001b[39m{:.4}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39mSell signal: \u001b[39;49m\u001b[39m{:.4}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(t,actions[\u001b[39m0\u001b[39;49m],actions[\u001b[39m1\u001b[39;49m],actions[\u001b[39m2\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/Documents/GitHub/RL_trading/DQN_RL/train.ipynb#ch0000002?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39margmax(actions): logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00maction_dict[action]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is an exploration.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/Documents/GitHub/RL_trading/DQN_RL/train.ipynb#ch0000002?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Precision not allowed in integer format specifier"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for e in range(1,num_episode +1):\n",
    "    logging.info(f'n\\nEpisode: {e}/{num_episode}')\n",
    "\n",
    "    agent.reset()\n",
    "    state = generate_combined_state(0,window_size,stock_prices,agent.balance,len(agent.inventory))\n",
    "\n",
    "    for t in range(1,trading_period+1):\n",
    "        if t % 100 ==0:\n",
    "            logging.info(f\"\\n -----------------------------Period: {t}/{trading_period}---------------\")\n",
    "\n",
    "            reward = 0\n",
    "            next_state= generate_combined_state(t,window_size,stock_prices,agent.balance,len(agent.inventory))\n",
    "            previous_portfolio_vlaue = len(agent.inventory) * stock_prices[t] +agent.balance\n",
    "\n",
    "            if model_name == \"DDPG\":\n",
    "                actions = agent.act(state,t)\n",
    "                action = np.argmax(actions)\n",
    "            else:\n",
    "                actions = agent.model.predict(state)[0]\n",
    "                action = agent.act(state)\n",
    "            print(t,actions)\n",
    "            logging.info(\"Step: 『』\\tHold signal: {:.4} \\t Buy signal: {:.4} \\tSell signal: {:.4}\".format(t,actions[0],actions[1],actions[2]))\n",
    "            if action != np.argmax(actions): logging.info(f\"\\t\\t'{action_dict[action]}' is an exploration.\")\n",
    "            if action ==0:\n",
    "                execution_result = hold(actions)\n",
    "            if action ==1:\n",
    "                execution_result =buy(t)\n",
    "            if action ==2:\n",
    "                execution_result = sell(t)\n",
    "\n",
    "            #checking execution result\n",
    "            if execution_result is None:\n",
    "                reward -=treasury_bond_daily_return_rate() * agent.balance #missing opportunity\n",
    "            else:\n",
    "                if isinstance(execution_result,tuple): #if execution_result is 'Hold'\n",
    "                    actions = execution_result[1]\n",
    "                    execution_result = execution_result[0]\n",
    "\n",
    "                logging.info(execution_result)\n",
    "\n",
    "            #calculate reward\n",
    "            current_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance    \n",
    "            unrealized_profit = current_portfolio_value - agent.initial_portfolio_value\n",
    "            reward += unrealized_profit\n",
    "\n",
    "            agent.portfolio_values.append(current_portfolio_value)\n",
    "            agent.return_rate.append((current_portfolio_value-previous_portfolio_vlaue)/previous_portfolio_vlaue)\n",
    "\n",
    "            done = True if t == trading_period else False   \n",
    "            agent.remember(state,actions,reward,next_state,done)\n",
    "\n",
    "            #udpate state\n",
    "            state = next_state\n",
    "            \n",
    "            #experience replay\n",
    "            if len(agent.memory) >agent.buffer_size:\n",
    "                num_experience_replay += 1\n",
    "                loss = agent.experience.replay()\n",
    "                #{:.2f} with 2 decimal place\n",
    "                logging.info(\"Episode: {}\\t Loss: {:.2f}\\tAction: {}\\t Reward:{:.2f}\\t Number of Stocks: {}\".format(e,loss,agent.balance,len(agent.inventory)))\n",
    "                agent.tensorboard.on_batch_end(num_experience_replay,{'loss':loss,\"portfolio value\": current_portfolio_value})\n",
    "            \n",
    "            if done:\n",
    "                portfolio_retunr = evaluate_portfolio_performance(agent,logging)\n",
    "                returns_across_episodes.append(portfolio_return)\n",
    "\n",
    "        if e%5 == 0:\n",
    "            if model_name ==\"DQN\":\n",
    "                agent.model.save(\"saved_models/DQN_ep\"+str(e)+\".h5\")\n",
    "            elif model_name == \"DDPG\":\n",
    "                agent.actor.model.save_weights(\"saved_models/DDPG_ep{}_actor.h5\".format(str(e)))\n",
    "                agent.critic.model.save_weights(\"saved_models/DDPG_ep{}_critic\".format(str(e)))\n",
    "\n",
    "            logging.info(\"model saved\")\n",
    "\n",
    "    logging.info('total training time: {0:.2f} min'.format((time.time()-start_time)/60))\n",
    "\n",
    "    plot_portfolio_returns_across_episodes(model_name, returns_across_episodes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
