{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Albert-Z-Guo/Deep-Reinforcement-Stock-Trading/tree/master/agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/97856004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在\n",
    "DQN是解决什么问题？\n",
    "\n",
    "DQN 解决的是 最优化决策 \n",
    "问题的变量是哪些？\n",
    "上一次决策给出的回报，过去的决策给出的回报\n",
    "\n",
    "优化的操作是：\n",
    "1. 初始化一个可能的任意固定值\n",
    "2. 每个时间t，agent 选择一个动作a_t， 得到一个奖励 R_t\n",
    "3. 进入一个新状态 S_t+1\n",
    "4. 更新Q值\n",
    "5. \n",
    "\n",
    "输出是：\n",
    "给出上一次决策，和由上一次决策得到的目前的情况，最优的决策是什么\n",
    "最终会找到一个最优策略， 即从当前状态开始，所有连续步骤的总回报为回报期望值的最大值是可以实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q table 告诉我们，在每一种情况下， 每一种行动的价值有多大\n",
    "适合小规模问题，大规模问题会遇到维度灾难和检索困难。\n",
    "\n",
    "\n",
    "\n",
    "神经网络中， Q table体现为一个函数，它的功能和Q table 一样，但是这个函数并没有具体地与Q table有映射关系，只是能够给出在同样情况下（state 和 action) 相似的reward 值\n",
    "\n",
    "% 暂时的理解\n",
    "\n",
    "输入数据为当前state， 输出数据为对于同一state，所有action的q value（输出为一向量）， 需要将该target Qtable 去匹配 现在输出的Q table\n",
    "\n",
    "每次输出一个q vector of a situation，一个reward 和一个新的situation。 然后用这个q vector 去匹配 reward 和discounted q value 的和。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Getting Started](./images/DQN%20algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expereince replay 的好处\n",
    "1. 因原输入数据 前后具有连续性， 会导致算法在连续一段时间内朝着同一方向做梯度下降，这样gradient有可能不收敛。 expreince replay 打破该连续性\n",
    "\n",
    "希望训练数据更加diverse， which leads to different strategy and different results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from utils import Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from utils import Portfolio\n",
    "\n",
    "class Agent(Portfolio):\n",
    "    def __init__(self,state_dim,balance,is_eval=False,model_name=\"\"):\n",
    "        super().__init__(balance=balance)\n",
    "        self.model_type=\"DQN\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = 3\n",
    "        self.memory=deque(maxlen=100)\n",
    "        self.buffer_size=60\n",
    "\n",
    "        self.gamma=0.95\n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_min= 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.is_eval = is_eval\n",
    "        self.model = load_model(\"save_models/{}.h5\".format(model_name)) if is_eval else self.model()\n",
    "        self.print_f()\n",
    "        \n",
    "        \n",
    "    def print_f(self):\n",
    "        print(\"hello world.\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def model(self):\n",
    "        print(\"test\")\n",
    "        model=Sequential()\n",
    "        model.add(Dense(units=64,input_dim = self.state_dim,activation=\"relu\"))\n",
    "        model.add(Dense(units =32,activation=\"relu\"))\n",
    "        model.add(Dense(units = 8,activation='relu'))\n",
    "        model.add(Dense(self.action_dim,activation='softmax'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.01))\n",
    "        return model\n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_portfolio()\n",
    "        self.epsilon=1.0\n",
    "\n",
    "    def remember(self,state,actions,reward,next_state,done):\n",
    "        self.memory.append((state,actions,reward,next_state,done))\n",
    "\n",
    "    def act(self,state):\n",
    "        if not self.is_eval and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        options = self.model.predict(state)\n",
    "        return np.argmax(options[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "\n",
    "        mini_batch = [self.memory[i] for i in range(len(self.memory)-self.buffer_size+1,len(self.memory))]\n",
    "\n",
    "        for state, actions, reward, next_state, done in mini_batch:\n",
    "            if not done:\n",
    "                Q_target_value = reward+self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                Q_target_value = reward\n",
    "\n",
    "            next_actions = self.model.predict(state)\n",
    "            next_actions[0][np.argmax(actions)]=Q_target_value\n",
    "            history = self.model.fit(state,next_actions,epochs=1,verbose =1)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return history.history['loss'][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "hello world.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "demo  = Agent(13,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world.\n"
     ]
    }
   ],
   "source": [
    "demo.print_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
