{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DeepNeuralAI/RL-DeepQLearning-Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. model\n",
    "2. data process\n",
    "3. training\n",
    "4. evaluate\n",
    "5. trade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import keras.backend as K \n",
    "from keras.models import Sequential,load_model,clone_model\n",
    "from keras.layers import Dense \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber \n",
    "from src.utils import timestamp\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    def __init__(self,state_size,model_type=\"ddqn\",pretrained=False,model_name=None,window_size=10,\n",
    "    reset_target_weight_interval=10):\n",
    "        self.model_type=model_type\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size =3\n",
    "        self.inventory = []\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.start = True\n",
    "\n",
    "        self.model_name =model_name\n",
    "        self.gamma = 0.99\n",
    "        self.rar = 0.99 # the rate controls the degree of exploration , larger ,more exploration\n",
    "        self.eps_min = 0.01\n",
    "        self.radr = 0.995\n",
    "        self.lr = 1e-5\n",
    "        self.loss=Huber\n",
    "        self.custom_objects = {\"huber\":Huber}\n",
    "        self.optimizer = Adam(lr=self.lr)\n",
    "        self.window_size = window_size\n",
    "\n",
    "        if pretrained and self.model_name is not None:\n",
    "            self.model = self.load()\n",
    "        else:\n",
    "            self.model = self.model_()\n",
    "        self.n_iter = 1\n",
    "        self.reset_interval = reset_target_weight_interval\n",
    "\n",
    "        self.target_model = clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        def load(self):\n",
    "            #optimizer only stores the lr coefficient, it will not affect the value of parameters\n",
    "            model = load_model(f\"models/{self.model_name}\",custom_objects = self.custom_objects,compile = False)\n",
    "            model.compile(optimizer = self.optimizer, loss= self.loss())\n",
    "            return model\n",
    "        \n",
    "        def save(self,episode):\n",
    "            if self.model_name is None:\n",
    "                self.model_name = f'{self.model_type}_{timestamp()}'\n",
    "            self.model.save(f\"models/{self.model_name}_{episode}\")\n",
    "\n",
    "        def model_(self):\n",
    "            model =Sequential()\n",
    "            model.add(Dense(units=256,activation=\"relu\",input_shape=(self.state_size,)))\n",
    "            model.add(Dense(units=512,activation=\"relu\"))\n",
    "            model.add(Dense(units=512,activation=\"relu\"))\n",
    "            model.add(Dense(units=256,activation=\"relu\"))\n",
    "            model.add(Dense(units=self.action_size))\n",
    "\n",
    "            model.compile(optimizer=self.optimizer,loss=self.loss())\n",
    "\n",
    "            return model\n",
    "\n",
    "        def action(self,state,evaluation=False):\n",
    "            #if first step, no action needed\n",
    "            #if it happens to be exploration , it return a action randomly\n",
    "            #if it happens to be a exploitation, it return an action with highest probability to reward.\n",
    "\n",
    "            if self.start:\n",
    "                self.start = False\n",
    "                return 1\n",
    "\n",
    "            if not evaluation and (random.random() <= self.rar):\n",
    "                return random.randrange(self.action_size)\n",
    "\n",
    "            action_probs  = self.model.predict(state)\n",
    "            return np.argmax(action_probs[0])\n",
    "        \n",
    "        def replay(self,batch_size):\n",
    "            \n",
    "            mini_batch = random.sample(self.memory,batch_size)\n",
    "            X_train,y_train = [],[]\n",
    "\n",
    "            if self.model_type == \"ddqn\":\n",
    "\n",
    "                if self.n_iter % self.reset_interval == 0:\n",
    "                    print(\"Setting Target Weights...\")\n",
    "\n",
    "                    self.target_model.set_weights(self.model.get_weights())\n",
    "                \n",
    "                for state,action,reward,next_state,done in mini_batch:\n",
    "                    if done:\n",
    "                        target = reward\n",
    "                    else:\n",
    "                        target = reward+self.gamma*self.target_model.predict(next_state)[0][np.argmax(self.model.predict(next_state)[0])]\n",
    "\n",
    "                    q_values = self.model.predict(state)\n",
    "                    q_values[0][action] = target\n",
    "                    X_train.append(state[0])\n",
    "                    y_train.append(q_values[0])\n",
    "\n",
    "            if self.rar > self.eps_min:\n",
    "                self.rar *= self.radr\n",
    "\n",
    "            loss = self.model.fit(\n",
    "                x = np.array(X_train),\n",
    "                y = np.array(y_train),\n",
    "                epochs =1,\n",
    "                verbose = 0,\n",
    "            ).history[\"loss\"][0]\n",
    "\n",
    "            return loss\n",
    "\n",
    "        def remember(self,state,action,reward,next_state,done):\n",
    "            self.memory.append((state,action,reward,next_state,done))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
